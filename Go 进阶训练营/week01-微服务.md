### 单体服务的困局  

1.尽管也是模块化逻辑，但是最终它还是会打包并部署为单体式应用。其中最主要问题就是这个应用太复杂，以至于任何单个开发者都不可能搞懂它。

2.应用无法扩展，可靠性很低

3.最终，敏捷性开发和部署变的无法完成。

所以微服务产生了，解决的思路就是：*化繁为简，分而治之*

### 微服务起源

* SOA（面向服务）是啥

  * 服务拆分后比较小，BUG 少，容易测试和维护，也容易扩展

  * **单一职责，**一个服务只做一件事情
  * **尽可能的早的创建原型，**先定义 API，达成契约
  * **可移植性比效率更重要，**通讯协议的可移植性更加重要

* SOA 和 微服务 是什么关系？

  * 你可以把微服务想成是 SOA 的一种实践。

* 微服务是什么？

  * **围绕业务功能构建的，服务关注单一业务，服务间采用轻量级的通信机制，可以全自动独立部署，可以使用不同的编程语言和数据存储技术。**

### 微服务的优缺点

- 优点

  - 服务拆分后比较小，BUG 少，容易测试和维护，也容易扩展
  - **原子服务，**一个服务只做一件事情，并且这个属于这个服务的也不应该拆分到其他服务去
  - **独立进程，**一个服务只有一个独立进程，可以很好的和当前的容器化进行结合，无状态的服务可以很容易的享受到，k8s 上的故障转移，自动重启等好处
  - **隔离部署，**每个服务之间独立部署，可以避免相互影响，并且和按需进行分配资源，节省成本
  - 去中心化服务治理
    - 数据去中心化，每个服务独享数据库，缓存等设施，也有个别情况多个服务共享数据库，例如面向用户的管理后台和面向管理员的管理后台
    - 治理去中心化
    - 技术去中心化，每个服务可以使用适合自己的技术进行实施，但是注意如果技术栈过于发散对于企业或者团队本身也是不利的

- 缺点

  - 服务之间的依赖关系复杂

    ，成千上万个服务相互依赖就像一团乱麻一样，剪不断理还乱。

    - 常见的解决方案：全链路追踪，例如， opentracing

  - 微服务本身是分布式系统，需要使用 RPC 或者 消息进行通信，此外，必须要写代码来处理消息传递中速度过慢或者服务不可用等局部失效问题

    - 例子：服务调用流量会容易被放大，如果 服务 A -> B ->C 如果 A 有一个循环调用 B，B 也有一个循环调用 C，那么一个请求到达 C 之后就被放大了 100 倍甚至上千倍。这是扛不住的
    - **常见解决方案：粗粒度的进程间通信（batch 接口，批量请求，避免 n+1 问题），隔离，超时保护，负载保护，熔断、限流、降级、重试，负载均衡**

  - 会有分布式事务问题，

    因为现在每个微服务之间都会有一个独立的数据库，事务在单体应用中很好处理，但是在跨服务时会变得很麻烦

    - 常见解决方案：两阶段提交、TCC 等
    - [小米信息部技术团队: 分布式事务，这一篇就够了](https://xiaomi-info.github.io/2020/01/02/distributed-transaction/)

  - 测试会非常复杂，

    由于依赖多，无法得知是因为功能异常还是依赖的某个服务发版出现问题

    - 常见解决方案：独立测试环境，后面会有一个解决方案

  - 服务模块间的依赖，应用的升级有可能会波及多个服务模块的修改。

    - 切记，在服务需要变更时我们要特别小心，服务提供者的变更可能引发服务消费者的兼容性破坏，**时刻谨记保持服务契约(接口)的兼容性**
    - 发送时要保守，接收时要开放。按照伯斯塔尔法则的思想来设计和实现服务时，发送的数据要更保守，意味着最小化的传送必要的信息，接收时更开放意味着要最大限度的容忍冗余数据，保证兼容性。

  - 对运维和基础建设的要求很高，

    基础设施需要自动化，日志采集，监控数据采集，告警，CICD，K8s 等

    - 常见解决方案：上云

### 微服务如何构建？

传统实现组件的方式是通过库(library)，库是和应用一起运行在进程中，库的局部变化意味着整个应用的重新部署。 通过服务来实现组件，意味着将应用拆散为一系列的服务运行在不同的进程中，那么单一服务的局部变化只需重新部署对应的服务进程。我们用 Go如何 实施一个微服务：

> 多个微服务组合(compose)完成了一个完整的用户场景(usecase)。

- kit：一个微服务的基础库(框架)。
- service：业务代码 + kit 依赖 + 第三方依赖组成的业务微服务
- rpc + message queue：轻量级通讯



### 微服务网关 API- Gateway 设计

![01_Go进阶训练营_微服务_v1.svg](https://img.lailin.xyz/image/1606552913072-f2ae26c9-6897-4dc8-9384-95f493ff5006.svg)

流量链路是什么？

- 移动端 -> API Gateway -> BFF -> 微服务
- 不含 CDN、负载均衡（LB）
- BFF 纯 web 的业务一般用 nodejs 做 SSR

为什么我们的服务不直接对外进行暴露？

- 前端（移动端、客户端、web）同学非常痛苦，需要对接多个服务，兼容性差，沟通效率低
- 后端同学也很痛苦，一年前的版本都有人使用，服务无法进行重构升级

为什么需要最外层的 api gateway?

- 基础库的同学非常痛苦，限流熔断安全等业务无关的功能需要进行升级的时候升不动
-  客户端到微服务直接通信，强耦合。
- **需要多次请求，客户端聚合数据，工作量巨大，延迟高**
- 协议不利于统一，各个部门间有差异，需要前端来兼容
- *面向**端***的**API**适配，耦合到了内部服务
- 多终端兼容逻辑复杂，每个服务都需要处理
- 统一逻辑无法收敛，比如安全认证、限流。

API Gateway 的优点

* 轻量交互：协议精简、聚合
* 差异服务：数据裁剪以及聚合、针对终端定制化API

* 动态升级：原有系统兼容升级，更新服务而非协议

* 沟通效率提升，协作模式演进为移动业务**+**网关小组

### 微服务如何拆分？

- 在对业务领域不是特别熟悉的时候，按照

  部门职能进行划分，例如账号、财务等

  - 注意划分的时候**要闭环**，不要相同的功能散落到几个部门当中

- 在系统稳定之后，积累了相关的业务经验和微服务开发经验之后，再考虑使用 DDD 限界上下文进行划分

- 如果可以闭环的解决一个用户场景，那么它应该是一个微服务

- 还可以根据访问频率进行区分划分，将用户高频访问的部分划分为一个服务

- 还可以根据读写进行划分

  - CQRS: 将应用程序分为两部分：命令端和查询端。命令端处理程序创建，更新和删除请求，并在数据更改时发出事件。查询端通过针对一个或多个物化视图执行查询来处理查询，这些物化视图通过订阅数据更改时发出的事件流而保持最新

![01_Go进阶训练营_微服务_v1.svg](https://img.lailin.xyz/image/1606631756304-ac2b31e1-771a-45e7-9b99-5234badc5d0d.svg)

### 微服务之间的安全设计

![01_Go进阶训练营_微服务_v1.svg](https://img.lailin.xyz/image/1606632869613-c887da17-ec2f-4221-822a-84bbd21e4d3f.svg)

在内网主要看安全级别一般有三种：

- Full Trust：假定内网服务之间是安全的，在内网裸奔
- Half Trust：内网服务之间需要进行认证鉴权，但是不需要所有的都进行加密
- Zero Trust: 零信任，任务内部网络是不安全的，类似公网，所有的请求通过身份认证鉴权之后，都需要通过安全加密，防止被嗅探
  - https://www.microsoft.com/en-us/security/business/zero-trust

### 微服务直接为什么要用grpc

- 为什么采用 gRPC?

  - **多语言：**语言中立，支持多种语言。

  - **轻量级、高性能：**序列化支持 PB(Protocol Buffer)和 JSON，PB 是一种语言无关的高性能序列化框架。

  - **可插拔**

  - **IDL：**基于文件定义服务，通过 proto3 工具生成指定语言的数据结构、服务端接口以及客户端 Stub。

  - 移动端：基于**标准的 HTTP2 设计，支持双向流、消息头压缩、单 TCP 的多路复用**、服务端推送等特性，这些特性使得 gRPC 在移动端设备上更加**省电和节省网络流量**。

  - **服务而非对象、消息而非引用：**促进微服务的系统间粗粒度消息交互设计理念。

  - **负载无关的：**不同的服务需要使用不同的消息类型和编码，例如 protocol buffers、JSON、XML 和 Thrift。

  - 流：Streaming API。

  - **阻塞式和非阻塞式：**支持异步和同步处理在客户端和服务端间交互的消息序列。

  - 元数据交换：

    常见的横切关注点，如认证或跟踪，依赖数据交换。

    - metadata

  - **标准化状态码：**客户端通常以有限的方式响应 API 调用返回的错误。

- 为什么不使用 restful

  - 每个客户端都需要单独写 SDK，复杂麻烦
  - 需要单独写文档，常常会因为代码更新了但是文档没更新陷入坑中
  - 性能不太好，json 传递相对于 pb 更耗流量，性能更低
  - http1.1 是一个单连接的请求，在内部网络环境，使用 http 比较浪费
  - **restful 是一个松散约束的协议，非常灵活，每个人，每个团队出来的代码都不太一样，比较容易出错**

### grpc 自带健康检查功能

gRPC 有一个标准的健康检测协议，在 gRPC 的所有语言实现中基本都提供了生成代码和用于设置运行状态的功能。

主动健康检查 health check，可以在服务提供者服务不稳定时，被消费者所感知，临时从负载均衡中摘除，减少错误请求。当服务提供者重新稳定后，health check*成功，重新加入到消费者的负载均衡，恢复请求。

也就是说：**当Consumer检测到Provider 不健康时，即使其存在注册表里面，也不会去连接。**

health check，同样也被用于外挂方式的容器健康检测，或者流量检测**(k8s liveness & readiness)**。

外挂方式就是：又一个第三方会检测Provider的健康状态。

![image-20210807125738209](/Users/zhuyaguang/Library/Application Support/typora-user-images/image-20210807125738209.png)

### 服务发现

![image.png](https://img.lailin.xyz/image/1606744144537-0fe881ca-5716-4b80-8800-0dee0a266c5f.png)

客户端发现

- 直连，比服务端服务发现少一次网络跳转
- Consumer 需要内置特定的服务发现客户端和发现逻辑
  - 可以将负载均衡逻辑下放到 sidecar 中进行解耦

服务端发现

- Consumer 无需关注服务发现具体细节，只需知道服务的 DNS 域名即可

- 支持异构语言开发，需要基础设施支撑，多了一次网络跳转，可能有性能损失，基础设施会比较复杂

  > B 站这边采用的是客户端发现的模式，我们公司用的更多的是服务端发现的模式

### 注册中心

**CP、CA、还是 AP**
实际场景是海量服务发现和注册，服务状态可以弱一致, 需要的是 AP 系统，只需最终一致性即可

- 注册的事件延迟
  - 高可用的服务在这方面问题不大
- 注销的事件延迟
  - 因为有上文提到的健康检查的机制，即使注销延迟，客户端也会主动的将节点移除
- 相关文档
  - 多个注册中心的对比：https://developer.aliyun.com/article/698930
  - https://nacos.io/zh-cn/index.html 阿里开源的注册中心，毛老师推荐 😂

**eureka 实现原理**

> b 站仿照用 go 写了一个 https://github.com/bilibili/discovery

**[![image.png](https://img.lailin.xyz/image/1606745349658-c7638390-9cbd-4eca-b2ad-4baa15449c3f.png)](https://img.lailin.xyz/image/1606745349658-c7638390-9cbd-4eca-b2ad-4baa15449c3f.png)**

**[image.png](https://img.lailin.xyz/image/1606745349658-c7638390-9cbd-4eca-b2ad-4baa15449c3f.png)**



- 服务注册
  - **注册：**服务方启动后向注册中心任意一个节点发送注册请求，然后这个节点会向其他节点进行广播同步
  - **心跳：**注册后定期（30s）向注册中心发送心跳
  - **下线：**下线时向注册中心发送下线请求
  - 注意：注册中心节点启动时需要加载缓存进行预热，所以不建议这个时候服务进行重启或者是发版
- 服务发现
  - 消费者定期向注册中心长轮训获取节点信息，获取到之后缓存到本地
- 网络故障
  - 服务方与注册中心
    - 注册中心会定期（60s）检测已失效（90s 未更新）的实例，失效之后就会移除，但是如果短时间内丢失大量心跳连接，（15min 内心跳低于期望值的 85%）就会开启自我保护模式，保留过期的服务不会进行删除
  - 注册中心与消费者
    - 消费者本地有缓存，问题不大
  - 服务方与消费者
    - 有健康检查，健康检查不通过时，会从消费者本地负载均衡中移除
- 注册中心故障
  - 不建议这个时候服务进行重启或者是发版，因为这个时候注册不上，会导致服务不可用，不发版短时间没有影响
  - 如果全部挂掉，启动时必须要等两到三个心跳周期，等所有的服务都注册上之后再开始提供服务运行消费者拉取数据
  - 如果挂掉一个，需要等其他的节点将信息同步到本机之后再提供服务
  - 数据同步时会对比时间戳，会保证当前节点的数据是最新的

### 多集群

> 注意这里的多集群都是单个机房内的

#### 多集群需求从何而来？

对于类似账号服务的 L0 级别的服务，几乎所有的服务都有依赖，需要尽可能的提高服务的可用性

- 从单一集群考虑，多个节点保证可用性，我们通常使用 N+2 的方式来冗余节点。
  - N 一般通过压测得出
- 从单一集群故障带来的影响面角度考虑冗余多套集群。
  - 例如依赖的 redis 出现问题，整个集群挂掉了
- 单个机房内的机房故障导致的问题
  - 多机房部署，如果在云上可能是多个可用区

#### 什么是多集群？

- 给某个服务部署多套，每一套都拥有独立的缓存，物理上相当于有多套资源，逻辑上划分为不同的集群，在服务注册的时候向注册中心注册的时候携带相关的集群标签

#### 如何降低健康检查流量

对于账号这种大量服务依赖的服务，仅仅是健康检查流量就会导致 30%以上的资源占用（B 站之前的真实情况）
可以使用子集算法，将后端的节点均分给所有的客户端

- 通常 20-100 个后端，部分场景需要大子集，比如大批量读写操作。
- 后端平均分给客户端。
- 客户端重启，保持重新均衡，同时对后端重启保持透明，同时连接的变动最小
  - 消费者变化的时候需要 重新平衡

```go
// from google sre
func Subset(backends []string, clientID, subsetSize int) []string {
	subsetCount := len(backends) / subsetSize

	// Group clients into rounds; each round uses the same shuffled list:
	round := clientID / subsetCount

	r := rand.New(rand.NewSource(int64(round)))
	r.Shuffle(len(backends), func(i, j int) { backends[i], backends[j] = backends[j], backends[i] })

	// The subset id corresponding to the current client:
	subsetID := clientID % subsetCount

	start := subsetID * subsetSize
	return backends[start : start+subsetSize]
}
Copy
```

**为什么上面这个算法可以保证可以均匀分布？**
首先，**shuffle 算法保证在 round 一致的情况下，backend 的排列一定是一致的。**
因为每个实例拥有从 0 开始的连续唯一的自增 id，且计算过程能够保证每个 round 内所有实例拿到的服务列表的排列一致，因此在同一个 round 内的 client 会分别 backend 排列的不同部分的切片作为选中的后端服务来建立连接。
所以只要 client id 是连续的，那么 client 发向 后端的连接就一定是连续的
**参考资料:**
https://sre.google/sre-book/load-balancing-datacenter/
https://xargin.com/limiting-conn-wih-subset/

### 多租户（如何解决多套测试环境的问题）

在一个微服务架构中**允许多系统共存**是利用微服务稳定性以及模块化最有效的方式之一，这种方式一般被称为多租户(multi-tenancy)。租户可以是测试，金丝雀发布，影子系统(shadow systems)，甚至服务层或者产品线，使用租户能够保证代码的隔离性并且能够基于流量租户做路由决策。

#### 如何解决测试环境的问题

> 场景:假设现在有一个服务调用链 A -> B -> C，如果 C 同时有多个同学开发，如果甲同学的代码正在测试中，但是乙同学不小心发了一个版本，就会导致甲同学的代码被冲掉，导致测试同学测着测着就出现 bug。测试同学无法得知这个问题是由于环境导致的还是代码缺陷。

##### 解决方案 1：多套物理环境（类似上文的多集群）

搭建多套测试环境，可以做到物理隔离，但是也会存在一些问题：

- 混用环境导致的不可靠测试。
- 多套环境带来的硬件成本。
- 难以做负载测试，仿真线上真实流量情况。
- 治标不治本，无法知道当前环境谁在使用，并且几套环境可以满足需求？万一又多几个人开发是不是又需要再来几套？

##### 解决方案 2：多租户，染色发布

![image.png](https://img.lailin.xyz/image/1606799997241-a37aa06a-4d44-4001-96bb-b0287e9eead5.png)



- 注册流程
  - 假设我们现在有一套问题的 `FAT1` 测试环境，然后现在对应用 B 做了修改
  - 开发同学通过发布平台发布一个新的 B 应用 B`，并且带上环境标签，例如:`red`
  - 应用 B`向注册中心进行注册时候会带上`red` 标签
  - 消费者 A 在向注册中心获取服务节点数据的时候也会获取到这个标签，并且在本地的负载均衡当中使用 `map[string]pool` 的结构进行保存
- 调用流程
  - 测试同学通过 A 进行调用测试，如果是 http 就在 header 中打上这个 `red` 标签，如果是 grpc 就在 metadata 中加入这个标签
  - A 调用 B 的时候，发现 header 中存在 `red` 标签就会去本地负载均衡查询，发现负载均衡中有 red 标签的连接，这个之后就直接调用到 B`，并且在调用的时候 A 会将 header 的标签信息进行透传
  - B` 收到请求之后，需要调用 C、D 这时候也是一样的会去负载均衡中进行查询，发现没有就会退回到默认的连接池中
- 如何进行联调？
  - 需要联调的应用打上相同的标签就可以了
- 注意事项
  - 应用版本发布时数据结构，例如 db 中的表，redis 中的 key，必须保证向下兼容
  - 测试的时候需要使用不同的测试账号
  - 注意来自外网的请求中的 header 必须删除，确保安全
- 核心思路
  - 跨服务传递请求携带上下文(context)，数据隔离的流量路由方案。
    - 根据标签进行流量路由，并且要确保可以透传
  - 利用服务发现注册租户信息，注册成特定的租户。
    - 发布部署平台需要支持方便启动多套环境，以及标签注入
    - 对不同的环境做了隔离，可以保证对关键业务没有影响

### 如何进行全链路压测

![image.png](https://img.lailin.xyz/image/1606802196792-9f605d74-4f29-458a-923f-6adc7a087e70.png)


和上面的测试环境的解决方案类似，但是我们需要搭建一套和线上一致的影子系统



- 如何解决压测数据对线上数据的影响
  - 基础设施需要做改造，采用同样的基础设施节点
  - 缓存：影子应用存储的数据放到影子库中，使用不同的 db
  - 数据库：自动将线上的数据结构复制一份到影子数据库中。里面的表结构保持一致，数据库名做一些变化，例如 db_shadow
  - 消息队列: 推送消息的时候使用不同的 topic 或者是携带一些 metadata 信息
- 需要提前做一些数据初始化的操作，提前进行准备
- 压测时携带压测标签，将流量自动路由到影子服务进行压测

这种方案同样可以用于灰度发版当中